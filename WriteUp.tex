\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[latin1]{inputenc}

\title{The Life of a Data Scientist}
\date{\today}
\author{Revekka Kostoeva, Nikita Vemuri, Kevin Qi,Nate Young, \\Yonatan Nozik}
\begin{document}
\maketitle

\section*{I. Introduction}
The Human Activity Recognition database was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with-mounted smartphone with embedded inertial sensors. The objective was to classify activities into one of the six activities performed: walking, walking upstairs, walking downstairs, sitting, standing, and laying down. The data set was gathered from a group of 30 volunteers, within the age bracket of 19-48 years of age. The obtained dataset has been randomly partitioned into two sets, where 70\% of the volunteers was selected for generating the training data and 30% the test data. 

Using Samsung Galaxy S II embedded accelerometer and gyroscope, data that was captured included 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters. The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.

Our team found this data set particularly interesting due to its possible applications in design of wearable technology, particularly with a focus on the medical realm. By designing an accurate model which could predict whether the readings gathered from a smartwatch were from a walking or a sitting activity, new applications are inevitable. For example, patients suffering from high blood pressure could be easily monitored by their doctors, without the blood pressure data being made vague from lack of activity information. With our model, a doctor could easily dispose of any data not recorded while the patient was sitting, and thus have a data set gathered from a single activity throughout the day from which the doctor could monitor the patient's blood pressure levels.

\section*{II. Data Preprocessing}
 	The data was split up into the 561 given features and the given activity labels for each feature. The labels for this data set are categorical and contain 6 possible types of activities, including walking, walking upstairs, walking downstairs, sitting, standing, and laying. 
 	
   \newline \newline To convert this data into a form usable for our classifiers, we mapped each of the activities into a class number 1-6. We did so by first converting the 6 different classes into one-hot vectors, and then took the argmax of each vector to get the numerical class label. Then these labels were used for the Logistic Regression and SVM models.
   
\section*{III. Data Modeling}
\subsection*{Feedforward Neural Network}
One model which was applied to this problem was a simple feedforward neural network model. The Keras implementation was used due to its simplicity and ease of use and configuration.

The parameters of the model underwent a large amount of tuning: ultimately it was found that the model performed best when the model had 10 layers or fewer; the final model had 7. These layers were all densely connected layers with an input layer size of 64 neurons and an output size of 6 neurons. The size of the hidden layers was tuned many times. In general the optimal size appeared to be a number of neurons between the input layer and the output layer. The final model used densely connected hidden layers of size 32, however there was minimal difference observed between hidden layers of other sizes, so long as they were within the bounds of the input and the output.

The aspect of the model which underwent the largest amount of tuning was the selection of the activation functions. The optimization of these functions was largely conducted through trial and error, although some examination of previous papers discussing this classification problem suggested some possible solutions. The most successful configuration proved to be layers with alternating ReLU and hyperbolic tangent activation functions ending with a softmax activation function. The inclusion of the softmax function proved to be critical: without a terminating softmax activation function, the model would virtually never proceed past approximately 20% accuracy. However, multiple softmax functions spread throughout the hidden layers resulted in a model which improved much more slowly across its iterations. Placing the softmax activation near the input layer of the model demonstrated a similar effect. As a result the final model included a single softmax activation at the output layer.

As for the other activation functions, alternating ReLU and hyperbolic tangent activations proved to be the most successful. Using other activation functions resulted in problems such as the model learning extremely slowly. Additionally, using exclusively ReLU or exclusively hyperbolic tangent activations resulted in the model becoming stuck below 20% accuracy and never improving. Exclusively using softmax actiation was similar, with peak accuracy of 18.22%.

Model optimizers did not prove to have a significant effect on the performance of the model. The most successful optimizers were stochastic gradient descent (SGD) and Adam. Adam performed slightly better than SGD but the difference was marginal, no larger than 1 or 2 percent in general. The model ultimately resulted in 95.66% accuracy. 

\subsection*{Logistic Regression}
For the logistic regression model, we used the LogisticRegression model from scikit learn with 6 classes. The default L2 penalty was used, which is computationally faster than L1 and gives a dense output. No other hyperparameters were tuned. This basic logistic regression model resulted in 96.1\% accuracy, which implies that the data is very linearly separable.

\subsection*{SVM}
From the logistic regression model, we learned that the data seems to be linearly separable, so when applying the SVM model, we chose to use a linear SVM despite having the option to use a nonlinear model. We used the SVM model from scikit learn with the probability option set to True. This enables estimates to be predicted using probability. By default the scikit learn SVM used the one-vs-one method when dealing with the multiple class labels. This resulted in a 96.4\% accuracy.
\section*{IV. Comparison of Models}
\section*{V. Discussion of Results}
The neural network resulted in a 95.66% accuracy, slightly lower than was expected. One possible reason for this is that when applying a feedforward neural network to time series data, an underlying trend in the data may exist which has a significant effect on the ability of the neural network to make predictions.

\section*{VI. Conclusion}




\end{document}